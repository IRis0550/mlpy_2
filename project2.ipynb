{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a9c673f4b97f45d0975f547cb6070545",
    "deepnote_cell_type": "markdown"
   },
   "source": "# Machine Learning in Python - Project 2\n\nDue Friday, April 12th by 4 pm.\n\n*Include contributors names in notebook metadata or here*",
   "block_group": "d12f5692dbf546e09c5366fefe31e721"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f6fb20cc81f34af892849ba423586298",
    "deepnote_cell_type": "markdown"
   },
   "source": "## Setup\n\n*Install any packages here and load data*",
   "block_group": "95b0187ca2a54027ab96f65f270ae13d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:36:16.360173Z",
     "start_time": "2024-04-08T19:36:16.338423Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    fico  dt_first_pi flag_fthb  dt_matr   cd_msa  mi_pct  cnt_units  \\\n0  709.0       201703         9   204702      NaN      12          1   \n1  649.0       201703         9   203202  33124.0       0          1   \n2  747.0       201703         9   203702  41180.0       0          1   \n3  711.0       201703         9   204702  20260.0       0          1   \n4  751.0       201703         N   204702      NaN      35          1   \n\n  occpy_sts  cltv  dti  ...  zipcode       id_loan  loan_purpose  \\\n0         P    84   26  ...    51300  F117Q1000376             N   \n1         P    52   22  ...    33100  F117Q1000418             C   \n2         I    43   20  ...    63100  F117Q1000479             N   \n3         I    80   21  ...    55800  F117Q1000523             P   \n4         P    95   24  ...    75900  F117Q1000719             P   \n\n  orig_loan_term cnt_borr    seller_name        servicer_name flag_sc  \\\n0            360        2  Other sellers      Other servicers     NaN   \n1            180        2  Other sellers      Other servicers     NaN   \n2            240        2  Other sellers      Other servicers     NaN   \n3            360        2  Other sellers      Other servicers     NaN   \n4            360        1  Other sellers  ARVESTCENTRALMTGECO     NaN   \n\n   prepaid default  \n0        0       1  \n1        1       0  \n2        1       0  \n3        1       0  \n4        1       0  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fico</th>\n      <th>dt_first_pi</th>\n      <th>flag_fthb</th>\n      <th>dt_matr</th>\n      <th>cd_msa</th>\n      <th>mi_pct</th>\n      <th>cnt_units</th>\n      <th>occpy_sts</th>\n      <th>cltv</th>\n      <th>dti</th>\n      <th>...</th>\n      <th>zipcode</th>\n      <th>id_loan</th>\n      <th>loan_purpose</th>\n      <th>orig_loan_term</th>\n      <th>cnt_borr</th>\n      <th>seller_name</th>\n      <th>servicer_name</th>\n      <th>flag_sc</th>\n      <th>prepaid</th>\n      <th>default</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>709.0</td>\n      <td>201703</td>\n      <td>9</td>\n      <td>204702</td>\n      <td>NaN</td>\n      <td>12</td>\n      <td>1</td>\n      <td>P</td>\n      <td>84</td>\n      <td>26</td>\n      <td>...</td>\n      <td>51300</td>\n      <td>F117Q1000376</td>\n      <td>N</td>\n      <td>360</td>\n      <td>2</td>\n      <td>Other sellers</td>\n      <td>Other servicers</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>649.0</td>\n      <td>201703</td>\n      <td>9</td>\n      <td>203202</td>\n      <td>33124.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>P</td>\n      <td>52</td>\n      <td>22</td>\n      <td>...</td>\n      <td>33100</td>\n      <td>F117Q1000418</td>\n      <td>C</td>\n      <td>180</td>\n      <td>2</td>\n      <td>Other sellers</td>\n      <td>Other servicers</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>747.0</td>\n      <td>201703</td>\n      <td>9</td>\n      <td>203702</td>\n      <td>41180.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I</td>\n      <td>43</td>\n      <td>20</td>\n      <td>...</td>\n      <td>63100</td>\n      <td>F117Q1000479</td>\n      <td>N</td>\n      <td>240</td>\n      <td>2</td>\n      <td>Other sellers</td>\n      <td>Other servicers</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>711.0</td>\n      <td>201703</td>\n      <td>9</td>\n      <td>204702</td>\n      <td>20260.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I</td>\n      <td>80</td>\n      <td>21</td>\n      <td>...</td>\n      <td>55800</td>\n      <td>F117Q1000523</td>\n      <td>P</td>\n      <td>360</td>\n      <td>2</td>\n      <td>Other sellers</td>\n      <td>Other servicers</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>751.0</td>\n      <td>201703</td>\n      <td>N</td>\n      <td>204702</td>\n      <td>NaN</td>\n      <td>35</td>\n      <td>1</td>\n      <td>P</td>\n      <td>95</td>\n      <td>24</td>\n      <td>...</td>\n      <td>75900</td>\n      <td>F117Q1000719</td>\n      <td>P</td>\n      <td>360</td>\n      <td>1</td>\n      <td>Other sellers</td>\n      <td>ARVESTCENTRALMTGECO</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data in easyshare.csv\n",
    "d = pd.read_csv(\"freddiemac.csv\")\n",
    "d.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:36:18.634016Z",
     "start_time": "2024-04-08T19:36:18.552838Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4883, 134) (1221, 134)\n",
      "Accuracy: 0.9836199836199836\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1201\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.98      1221\n",
      "   macro avg       0.49      0.50      0.50      1221\n",
      "weighted avg       0.97      0.98      0.98      1221\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1201    0]\n",
      " [  20    0]]\n",
      "ROC AUC Score: 0.7401748542880933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('freddiemac.csv')\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = data.drop('default', axis=1)\n",
    "y = data['default']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "# Filling missing numeric values with the median\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "X_train[numeric_cols] = X_train[numeric_cols].fillna(X_train[numeric_cols].median())\n",
    "X_test[numeric_cols] = X_test[numeric_cols].fillna(X_train[numeric_cols].median())\n",
    "\n",
    "# Encoding categorical variables using dummy encoding\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align the columns in X_train and X_test to ensure they have the same set of dummy variables\n",
    "X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 创建逻辑回归模型的实例\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 使用训练数据拟合模型\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate the classification report, handling cases of zero division\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "# Compute the confusion matrix and ROC AUC score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T19:42:10.142485Z",
     "start_time": "2024-04-08T19:42:09.459056Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "d.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data info\n",
    "d.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate the proportion of default == 1\n",
    "d['default'].mean()\n",
    "# really imbalanced data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# count the nunber of missing values\n",
    "d.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# drop the missing value in 'fico'\n",
    "d_new = d.dropna(subset=['fico'])\n",
    "# fill the missing value in 'cd_msa' and 'flag_sc' with 9\n",
    "d_new['cd_msa'] = d_new['cd_msa'].fillna(9)\n",
    "d_new['flag_sc'] = d_new['flag_sc'].fillna(9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# check the missing value again\n",
    "d_new.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a professional body (e.g. FreddiMac company or similar company). If you use any additional data sources, you should introduce them here and discuss why they were included.*\n",
    "\n",
    "*Briefly outline the approaches being used and the conclusions that you are able to draw.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling. Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.*\n",
    "\n",
    "*You should also split your data into training and testing sets, ideally before you look to much into the features and relationships with the target*\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data. Specifically, this should be any code that you use to generate new columns in the data frame `d`. Feature engineering that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "*If you decide to extract additional features from the full data (easyshare_all.csv), describe these variables here.*\n",
    "\n",
    "*All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Fitting and Tuning\n",
    "\n",
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model. You are strongly encouraged to explore many different modeling methods (e.g. linear regression, interaction terms, lasso, etc.) but you should not include a detailed narrative of all of these attempts. At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validating it.*\n",
    "\n",
    "*For example if you considered a linear regression model, a polynomial regression, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "*This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.*\n",
    "\n",
    "*Finally, you should also provide comparison of your model with baseline model(s) on the test data but only briefly describe the baseline model(s) considered*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion & Conclusions\n",
    "\n",
    "*In this section you should provide a general overview of your final model, its performance, and reliability. You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.*\n",
    "\n",
    "*This should be written with a target audience of a government official or charity directy, who is understands the pressing challenges associated with ageining and dementia but may only have university level mathematics (not necessarily postgraduate statistics or machine learning). Your goal should be to highlight to this audience how your model can useful. You should also mention potential limitations of your model.*\n",
    "\n",
    "*Finally, you should include recommendations on potential lifestyle changes or governmental/societal interventions to reduce dementia risk.*\n",
    "\n",
    "*Keep in mind that a negative result, i.e. a model that does not work well predictively, that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explinations / justifications.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "*Include references if any*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Run the following to render to PDF\n",
    "!jupyter nbconvert --to pdf project2.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a2a9ec8d-a343-4210-b36b-f9db26268fc5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_notebook_id": "d2f25b2b0abf4f65af08d733185ca9f1",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
